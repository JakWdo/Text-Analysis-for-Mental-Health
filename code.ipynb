{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6135912,"sourceType":"datasetVersion","datasetId":3518362},{"sourceId":10127973,"sourceType":"datasetVersion","datasetId":6250185}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n# Wczytanie danych\ndata = pd.read_csv(\"/kaggle/input/reddit-mental-health-data/data_to_be_cleansed.csv\")\n\n# Zakładamy, że kolumny to: \"#\", \"title\", \"text\", \"target\"\n# Kolumna \"#\" jest zbędna, więc ją ignorujemy\nprint(data.columns)\ndata = data.rename(columns={\"Unnamed: 0\":\"#\"})\ndata = data.drop(columns=['#'])\nprint(data.columns)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:02:46.094215Z","iopub.execute_input":"2024-12-07T11:02:46.094949Z","iopub.status.idle":"2024-12-07T11:02:46.280887Z","shell.execute_reply.started":"2024-12-07T11:02:46.094914Z","shell.execute_reply":"2024-12-07T11:02:46.280040Z"}},"outputs":[{"name":"stdout","text":"Index(['Unnamed: 0', 'text', 'title', 'target'], dtype='object')\nIndex(['text', 'title', 'target'], dtype='object')\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Łączenie 'title' i 'text' w jeden ciąg wejściowy\ndata[\"combined\"] = data[\"title\"].fillna(\"\") + \" \" + data[\"text\"].fillna(\"\")\n\n# Podział na zbiory (train, val, test)\nprint(\"[INFO] Dzielenie danych na zbiory...\")\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['target'])\ntrain_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42, stratify=train_data['target'])\n\nprint(f\"Rozmiar zbioru treningowego: {len(train_data)}\")\nprint(f\"Rozmiar zbioru walidacyjnego: {len(val_data)}\")\nprint(f\"Rozmiar zbioru testowego: {len(test_data)}\")\n\n# Przygotowanie wektorów TF-IDF\nprint(\"[INFO] Dopasowywanie TfidfVectorizer na zbiorze treningowym...\")\nvectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))  # można dostosować parametry\n\nX_train = vectorizer.fit_transform(train_data[\"combined\"])\ny_train = train_data[\"target\"]\n\nX_val = vectorizer.transform(val_data[\"combined\"])\ny_val = val_data[\"target\"]\n\nX_test = vectorizer.transform(test_data[\"combined\"])\ny_test = test_data[\"target\"]\n\n# Trening modelu LogisticRegression\nprint(\"[INFO] Trening modelu LogisticRegression...\")\nmodel = LogisticRegression(max_iter=1000, class_weight='balanced')\nmodel.fit(X_train, y_train)\n\n# Ewaluacja na zbiorze walidacyjnym\nprint(\"[INFO] Ewaluacja na zbiorze walidacyjnym...\")\nval_preds = model.predict(X_val)\nval_acc = accuracy_score(y_val, val_preds)\nval_f1 = f1_score(y_val, val_preds, average='weighted')\nprint(f\"Walidacja - Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n\n# Ewaluacja na zbiorze testowym\nprint(\"[INFO] Ewaluacja na zbiorze testowym...\")\ntest_preds = model.predict(X_test)\ntest_acc = accuracy_score(y_test, test_preds)\ntest_f1 = f1_score(y_test, test_preds, average='weighted')\nprint(f\"Test - Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n\nprint(\"[TEST] Szczegółowy raport klasyfikacji:\")\nprint(classification_report(y_test, test_preds, target_names=[\"Stress\",\"Depression\",\"Bipolar\",\"Personality\",\"Anxiety\"]))\n\nprint(\"[INFO] Proces zakończony pomyślnie.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\n# ********************** ETAP 1: WCZYTANIE DANYCH **********************\n\nprint(\"[INFO] Wczytywanie danych EmoBank...\")\ndata = pd.read_csv(\"/kaggle/input/emobank/emobank.csv\")  # Zakładamy, że plik jest TSV\n\n# Filtrujemy wiersze z pustym tekstem\ndata = data.dropna(subset=[\"text\"])\n\n# Podział wg split\ntrain_data = data[data[\"split\"] == \"train\"]\nval_data = data[data[\"split\"] == \"dev\"]\ntest_data = data[data[\"split\"] == \"test\"]\n\nprint(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n\n# ********************** ETAP 2: WEKTORYZACJA TEKSTU **********************\n\nprint(\"[INFO] Wektoryzacja tekstu za pomocą TF-IDF...\")\nvectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\nX_train = vectorizer.fit_transform(train_data[\"text\"])\nX_val = vectorizer.transform(val_data[\"text\"])\nX_test = vectorizer.transform(test_data[\"text\"])\n\ny_train = train_data[[\"V\", \"A\", \"D\"]].values\ny_val = val_data[[\"V\", \"A\", \"D\"]].values\ny_test = test_data[[\"V\", \"A\", \"D\"]].values\n\n# Konwersja do formatu odpowiedniego dla PyTorch\nX_train = X_train.toarray()\nX_val = X_val.toarray()\nX_test = X_test.toarray()\n\ny_train = y_train.astype(np.float32)\ny_val = y_val.astype(np.float32)\ny_test = y_test.astype(np.float32)\n\n# ********************** ETAP 3: DEFINICJA DATASET I DATALOADER **********************\n\nclass EmoBankDataset(Dataset):\n    def __init__(self, features, targets):\n        self.X = torch.tensor(features, dtype=torch.float32)\n        self.y = torch.tensor(targets, dtype=torch.float32)\n    \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# Tworzenie zestawów danych\ntrain_dataset = EmoBankDataset(X_train, y_train)\nval_dataset = EmoBankDataset(X_val, y_val)\ntest_dataset = EmoBankDataset(X_test, y_test)\n\n# Tworzenie DataLoaderów\nbatch_size = 32\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(\"[INFO] Zestawy danych przygotowane.\")\n\n# ********************** ETAP 4: DEFINICJA MODEL **********************\n\nclass EmotionRegressor(nn.Module):\n    def __init__(self, input_dim):\n        super(EmotionRegressor, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(128, 64)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(64, 3)  # Wyjście: V, A, D\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        return out\n\ninput_dim = X_train.shape[1]\nmodel = EmotionRegressor(input_dim)\n\n# Sprawdzenie, czy GPU jest dostępne\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(f\"[INFO] Używane urządzenie: {device}\")\n\n# ********************** ETAP 5: DEFINICJA STRATY I OPTYMIZATORA **********************\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# ********************** ETAP 6: TRENING MODEL **********************\n\nnum_epochs = 20\n\nprint(\"[INFO] Rozpoczynanie treningu...\")\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(train_loader):\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n    \n    avg_loss = running_loss / len(train_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] - Średnia strata: {avg_loss:.4f}\")\n    \n    # Walidacja po każdej epoce\n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            val_losses.append(loss.item())\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] - Strata walidacyjna: {avg_val_loss:.4f}\\n\")\n\nprint(\"[INFO] Trening zakończony.\")\n\n# ********************** ETAP 7: EWALUACJA NA ZBIORZE TESTOWYM **********************\n\nprint(\"[INFO] Ewaluacja na zbiorze testowym...\")\nmodel.eval()\ntest_predictions = []\ntest_targets = []\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        test_predictions.append(outputs.cpu().numpy())\n        test_targets.append(targets.numpy())\n\ntest_predictions = np.vstack(test_predictions)\ntest_targets = np.vstack(test_targets)\n\n# Obliczanie MSE dla każdego wymiaru\nmse_v = mean_squared_error(test_targets[:, 0], test_predictions[:, 0])\nmse_a = mean_squared_error(test_targets[:, 1], test_predictions[:, 1])\nmse_d = mean_squared_error(test_targets[:, 2], test_predictions[:, 2])\n\nprint(f\"Test MSE - Valence (V): {mse_v:.4f}\")\nprint(f\"Test MSE - Arousal (A): {mse_a:.4f}\")\nprint(f\"Test MSE - Dominance (D): {mse_d:.4f}\")\n\n# ********************** ETAP 8: ZAPIS MODELU I WEKTORYZERU **********************\n\nimport joblib\nimport os\n\nmodel_dir = \"trained_emobank_model\"\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n\n# Zapis modelu\ntorch.save(model.state_dict(), os.path.join(model_dir, \"emotion_regressor.pth\"))\nprint(f\"[INFO] Model zapisany w {os.path.join(model_dir, 'emotion_regressor.pth')}\")\n\n# Zapis wektoryzera\njoblib.dump(vectorizer, os.path.join(model_dir, \"tfidf_vectorizer.pkl\"))\nprint(f\"[INFO] Wektoryzer zapisany w {os.path.join(model_dir, 'tfidf_vectorizer.pkl')}\")\n\nprint(\"[INFO] Proces zakończony pomyślnie.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:25:49.108466Z","iopub.execute_input":"2024-12-07T11:25:49.108810Z"}},"outputs":[{"name":"stdout","text":"[INFO] Wczytywanie danych EmoBank...\nTrain: 8062, Val: 999, Test: 1000\n[INFO] Wektoryzacja tekstu za pomocą TF-IDF...\n[INFO] Zestawy danych przygotowane.\n[INFO] Używane urządzenie: cuda\n[INFO] Rozpoczynanie treningu...\nEpoch [1/20], Batch [100/252], Loss: 0.3551\nEpoch [1/20], Batch [200/252], Loss: 0.1128\nEpoch [1/20], Batch [252/252], Loss: 0.1106\nEpoch [1/20] - Średnia strata: 1.7269\nEpoch [1/20] - Strata walidacyjna: 0.1052\n\nEpoch [2/20], Batch [100/252], Loss: 0.0968\nEpoch [2/20], Batch [200/252], Loss: 0.1172\nEpoch [2/20], Batch [252/252], Loss: 0.0638\nEpoch [2/20] - Średnia strata: 0.0829\nEpoch [2/20] - Strata walidacyjna: 0.0888\n\nEpoch [3/20], Batch [100/252], Loss: 0.0615\nEpoch [3/20], Batch [200/252], Loss: 0.0819\nEpoch [3/20], Batch [252/252], Loss: 0.0599\nEpoch [3/20] - Średnia strata: 0.0663\nEpoch [3/20] - Strata walidacyjna: 0.0835\n\nEpoch [4/20], Batch [100/252], Loss: 0.0448\nEpoch [4/20], Batch [200/252], Loss: 0.0606\nEpoch [4/20], Batch [252/252], Loss: 0.0888\nEpoch [4/20] - Średnia strata: 0.0580\nEpoch [4/20] - Strata walidacyjna: 0.0826\n\nEpoch [5/20], Batch [100/252], Loss: 0.0492\nEpoch [5/20], Batch [200/252], Loss: 0.0393\nEpoch [5/20], Batch [252/252], Loss: 0.0699\nEpoch [5/20] - Średnia strata: 0.0491\nEpoch [5/20] - Strata walidacyjna: 0.0839\n\nEpoch [6/20], Batch [100/252], Loss: 0.0438\nEpoch [6/20], Batch [200/252], Loss: 0.0805\nEpoch [6/20], Batch [252/252], Loss: 0.0353\nEpoch [6/20] - Średnia strata: 0.0422\nEpoch [6/20] - Strata walidacyjna: 0.0828\n\nEpoch [7/20], Batch [100/252], Loss: 0.0436\nEpoch [7/20], Batch [200/252], Loss: 0.0236\nEpoch [7/20], Batch [252/252], Loss: 0.0507\nEpoch [7/20] - Średnia strata: 0.0355\nEpoch [7/20] - Strata walidacyjna: 0.0859\n\nEpoch [8/20], Batch [100/252], Loss: 0.0264\nEpoch [8/20], Batch [200/252], Loss: 0.0271\nEpoch [8/20], Batch [252/252], Loss: 0.0321\nEpoch [8/20] - Średnia strata: 0.0292\nEpoch [8/20] - Strata walidacyjna: 0.0881\n\nEpoch [9/20], Batch [100/252], Loss: 0.0251\nEpoch [9/20], Batch [200/252], Loss: 0.0277\nEpoch [9/20], Batch [252/252], Loss: 0.0462\nEpoch [9/20] - Średnia strata: 0.0242\nEpoch [9/20] - Strata walidacyjna: 0.0905\n\nEpoch [10/20], Batch [100/252], Loss: 0.0147\nEpoch [10/20], Batch [200/252], Loss: 0.0236\nEpoch [10/20], Batch [252/252], Loss: 0.0234\nEpoch [10/20] - Średnia strata: 0.0206\nEpoch [10/20] - Strata walidacyjna: 0.0942\n\nEpoch [11/20], Batch [100/252], Loss: 0.0099\nEpoch [11/20], Batch [200/252], Loss: 0.0529\nEpoch [11/20], Batch [252/252], Loss: 0.0173\nEpoch [11/20] - Średnia strata: 0.0182\nEpoch [11/20] - Strata walidacyjna: 0.0962\n\nEpoch [12/20], Batch [100/252], Loss: 0.0210\nEpoch [12/20], Batch [200/252], Loss: 0.0257\nEpoch [12/20], Batch [252/252], Loss: 0.0121\nEpoch [12/20] - Średnia strata: 0.0171\nEpoch [12/20] - Strata walidacyjna: 0.0992\n\nEpoch [13/20], Batch [100/252], Loss: 0.0153\nEpoch [13/20], Batch [200/252], Loss: 0.0198\nEpoch [13/20], Batch [252/252], Loss: 0.0266\nEpoch [13/20] - Średnia strata: 0.0152\nEpoch [13/20] - Strata walidacyjna: 0.1003\n\nEpoch [14/20], Batch [100/252], Loss: 0.0097\nEpoch [14/20], Batch [200/252], Loss: 0.0157\nEpoch [14/20], Batch [252/252], Loss: 0.0117\nEpoch [14/20] - Średnia strata: 0.0138\nEpoch [14/20] - Strata walidacyjna: 0.1028\n\nEpoch [15/20], Batch [100/252], Loss: 0.0107\nEpoch [15/20], Batch [200/252], Loss: 0.0129\nEpoch [15/20], Batch [252/252], Loss: 0.0120\nEpoch [15/20] - Średnia strata: 0.0128\nEpoch [15/20] - Strata walidacyjna: 0.1031\n\nEpoch [16/20], Batch [100/252], Loss: 0.0138\nEpoch [16/20], Batch [200/252], Loss: 0.0060\nEpoch [16/20], Batch [252/252], Loss: 0.0134\nEpoch [16/20] - Średnia strata: 0.0121\nEpoch [16/20] - Strata walidacyjna: 0.1013\n\nEpoch [17/20], Batch [100/252], Loss: 0.0076\nEpoch [17/20], Batch [200/252], Loss: 0.0086\nEpoch [17/20], Batch [252/252], Loss: 0.0182\nEpoch [17/20] - Średnia strata: 0.0118\nEpoch [17/20] - Strata walidacyjna: 0.1039\n\nEpoch [18/20], Batch [100/252], Loss: 0.0065\nEpoch [18/20], Batch [200/252], Loss: 0.0129\nEpoch [18/20], Batch [252/252], Loss: 0.0112\nEpoch [18/20] - Średnia strata: 0.0111\nEpoch [18/20] - Strata walidacyjna: 0.1043\n\n","output_type":"stream"}],"execution_count":null}]}